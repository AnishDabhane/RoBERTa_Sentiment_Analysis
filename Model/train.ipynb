{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import RobertaTokenizerFast, RobertaModel, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>aaplthe 10 best steve jobs emails ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>rt why aapl stock had a miniflash crash today ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>my cat only chews cords such an applesnob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>i agree with that the individualinvestor shoul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>nobody expects the spanish inquisition aapl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                         clean_text\n",
       "0        0.0             aaplthe 10 best steve jobs emails ever\n",
       "1        0.0  rt why aapl stock had a miniflash crash today ...\n",
       "2        0.0          my cat only chews cords such an applesnob\n",
       "3        0.0  i agree with that the individualinvestor shoul...\n",
       "4        0.0        nobody expects the spanish inquisition aapl"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../dataset/data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5622,) (993,)\n"
     ]
    }
   ],
   "source": [
    "data_train, data_val, y_err_train, y_err_val = train_test_split(\n",
    "                                                        data['clean_text'].values, \n",
    "                                                        data['sentiment'].values,\n",
    "                                                        test_size=0.15,\n",
    "                                                        random_state=42)\n",
    "print(data_train.shape, data_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(1, 19), dtype=int32, numpy=\n",
      "array([[    0,   405,  1364,   456, 26536,   254,    45,   145,    10,\n",
      "         1368,  3540,   114,    63,   129,    65,   621,  5278,   417,\n",
      "            2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 19), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "      dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 26), dtype=int32, numpy=\n",
      "array([[    0,  9713,  4800,  1289,  1597,   179,  3246,    11,  1400,\n",
      "           11,   295,   219,   438,    16,    24,   162,    50,    16,\n",
      "           42,  6378, 33540, 24352,   114,    47,  2854,     2]],\n",
      "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 26), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 17), dtype=int32, numpy=\n",
      "array([[    0, 39563,    13,   489, 13964,    77, 19116,  6773,    31,\n",
      "           10,   400,   543,  1305,     7,    10, 10228,     2]],\n",
      "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 17), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 13), dtype=int32, numpy=\n",
      "array([[    0,   118,    64,   192,    10,    92,     8,  2782,   741,\n",
      "        17283,    31,     8,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 13), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 12), dtype=int32, numpy=\n",
      "array([[    0,  5488,    16,   357,  1437, 28778,  1264,   195,   910,\n",
      "          195,    29,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 12), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 28), dtype=int32, numpy=\n",
      "array([[    0, 27326,  1400,  1926,  2850,  1242,  1872,   417,  3760,\n",
      "          921,  2850,  1242,  1872,  5610, 15001,    34,     5,  2373,\n",
      "         2111,   575,  1437,  2088,   655,   450,    11,   127,   301,\n",
      "            2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 28), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 23), dtype=int32, numpy=\n",
      "array([[    0,   225, 20768,   154,     5, 33605, 16849,    23,     5,\n",
      "         3665,  1709,   515,   103,  2770,  4803,  9406,   917,    11,\n",
      "            5,   929, 48726,   196,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 23), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 24), dtype=int32, numpy=\n",
      "array([[    0,  7333,   585,     5,  1437, 28778,  1264,   195,    29,\n",
      "         1437, 28778,  1264,   195,   438,   939,   366,   262,   800,\n",
      "         1248,  4312,  1437, 28778,  1264,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 24), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 22), dtype=int32, numpy=\n",
      "array([[    0, 16244,    31, 20150,  1139,  5179, 15162,   291,  4631,\n",
      "            5,   265,    14, 11235,   548,  1315,  1490,  1437, 28778,\n",
      "         1264, 47191,   406,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 22), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "      dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 23), dtype=int32, numpy=\n",
      "array([[    0, 10010,   300,    10, 44831,  2538,    11,    10, 11342,\n",
      "         7384,   596, 17672,    52,    33,    10,   579,  6377,    11,\n",
      "           10,   471, 25713,   506,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 23), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[   0, 1225,  424,    8,  291, 3822, 1597,    2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 18), dtype=int32, numpy=\n",
      "array([[    0,  6621,  2911, 27326,  5853,    10,   102,  2911,  5712,\n",
      "           13,   186,   142,     9,   475, 46328,   662,  1874,     2]],\n",
      "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 18), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "      dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 27), dtype=int32, numpy=\n",
      "array([[    0,  6920,   992, 21028,  2865,    29,  8946, 26908, 27171,\n",
      "        15679,  7142, 18408,   607, 17275,  1328,   777, 15162, 23473,\n",
      "         5946,    10,   102,  2911,    34,   551,    10,  1368,     2]],\n",
      "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 27), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 19), dtype=int32, numpy=\n",
      "array([[    0,  6968,   241,    45,   190,     5,   588, 23523, 15162,\n",
      "          138,    99,     5, 26536,   524,   939,  3518,     7,   109,\n",
      "            2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 19), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "      dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 16), dtype=int32, numpy=\n",
      "array([[    0,  6621,  2911, 19424,  3488, 20150,  1437, 28778,  1264,\n",
      "          231,   647,   903,     8,   364,  3275,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 16), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 24), dtype=int32, numpy=\n",
      "array([[    0,    29,  6841,    28,    15,    14,  8347, 15328,  2758,\n",
      "          162,    63,    45,  6908,  1895,     8, 15328,  2955, 26536,\n",
      "          939,   582,    13,    69,    13,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 24), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 22), dtype=int32, numpy=\n",
      "array([[    0,  3785,   173,    15,   184, 23199,  7465,    19,   546,\n",
      "            7,  2229,    53,   236,     7,   120,   402,    14,    16,\n",
      "          184, 23199,  1227,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 22), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "      dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[  0, 118, 761,   9, 802,  14, 137, 865,   2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=\n",
      "array([[    0, 17635, 40105,  1075,  6822,  1400,  1092,     2]],\n",
      "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 24), dtype=int32, numpy=\n",
      "array([[    0,  4544,   939,   366,   406, 15162, 15162,  1236,  6119,\n",
      "         2088, 11355, 32733, 11355,  4460, 40123,   605,  7915, 32696,\n",
      "          134,  8502,   288,   873, 42406,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 24), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[    0,   219,  1250, 43816,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 18), dtype=int32, numpy=\n",
      "array([[    0,  1594,    47,   146,   785,    14,  1108,   197,  3999,\n",
      "            5,   403,    28,   481,  1904, 14247, 42612,  4086,     2]],\n",
      "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 18), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "      dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 13), dtype=int32, numpy=\n",
      "array([[    0,   118,   524,  2157, 34208,  1690, 35276,    30,     5,\n",
      "          595, 21554,  4230,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 13), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 17), dtype=int32, numpy=\n",
      "array([[    0, 28481,    70,  5126,    11,     5,    10,   102,  2911,\n",
      "        12402,   119,  1500,    14,    16,  1747,  6159,     2]],\n",
      "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 17), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 16), dtype=int32, numpy=\n",
      "array([[    0, 25800,     5, 36180,   625,    16,   932,    53, 23326,\n",
      "           10,   102,  2911,    10,   102,  2911,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 16), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 22), dtype=int32, numpy=\n",
      "array([[    0, 30214,  1437, 28778,  1264,   231,   647,  1920,   939,\n",
      "          366,   210,   458,    62,  3612,   266,    10,   102,  2911,\n",
      "           10,   102,  2911,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 22), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "      dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 21), dtype=int32, numpy=\n",
      "array([[    0, 27326,  9379, 16014,  1437, 28778,  1264,   401,  1787,\n",
      "           13,    63,   308,  2326, 34154,   417, 45848,  1241,    10,\n",
      "          102,  2911,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 21), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "      dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 16), dtype=int32, numpy=\n",
      "array([[    0,   102,  1115, 14726, 14035,  1437, 28778,  1264,    13,\n",
      "        12373,     8,  1325,    62,     7, 21903,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 16), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 9), dtype=int32, numpy=\n",
      "array([[    0,  2629,    99,   144,     9,    84, 45993,   109,     2]],\n",
      "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 12), dtype=int32, numpy=\n",
      "array([[    0,   757,    23,  1400,   233,  8614, 23954,  3741, 10528,\n",
      "        19037, 11163,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 12), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 13), dtype=int32, numpy=\n",
      "array([[    0,   118, 33976,  1346,   596,   939,   524,    45,  4973,\n",
      "           13,    41,  6662,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 13), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 18), dtype=int32, numpy=\n",
      "array([[    0, 12196,     5, 17835,   127,  1028,  2441,  1326,   101,\n",
      "           24,   685,    10,  2168,    19,    10,  2508, 10868,     2]],\n",
      "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 18), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "      dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 18), dtype=int32, numpy=\n",
      "array([[    0, 36051, 45724,  8223,     5,  1104,   790, 29224, 13738,\n",
      "        22868,    19,    41,  1437, 28778,  1264,   231,    30,     2]],\n",
      "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 18), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "      dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[    0,  2527,    42,    16,     5,  2755,  8257,   639,     5,\n",
      "        33842,  1437, 28778,  1264,   231,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 16), dtype=int32, numpy=\n",
      "array([[    0, 46078,   910,    90,   939,  2489,  9399,  1276,   648,\n",
      "          939,   429,   120, 44980,    59,    24,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 16), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 18), dtype=int32, numpy=\n",
      "array([[    0, 28778,  1264,   401,   231,  2704,  7691, 12490,    11,\n",
      "        15162,   647,  3612,  3553, 46806,   204,   263, 47153,     2]],\n",
      "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 18), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "      dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 12), dtype=int32, numpy=\n",
      "array([[    0, 27326,  8635,   139, 15679,  7142,   606,    66,    25,\n",
      "         5100,  1241,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 12), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 12), dtype=int32, numpy=\n",
      "array([[    0,  6621,  2911, 27326,  1500,  1388,   396,    10, 19350,\n",
      "           13,   122,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 12), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 22), dtype=int32, numpy=\n",
      "array([[    0, 20565,   516, 12766,   293,    19,  1495,  1437, 28778,\n",
      "         1264,   401,  1954, 36180,   625,    54,   782,    41, 36180,\n",
      "          625,   143,  1181,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 22), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "      dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 20), dtype=int32, numpy=\n",
      "array([[    0,  9713,  4535,    23,  2248,   151, 16935,    65,  9455,\n",
      "          204,  7796,    52,   240,     5, 10535,     9,     5,   499,\n",
      "          122,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 20), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "      dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 19), dtype=int32, numpy=\n",
      "array([[    0, 27326, 19961,   261, 29522,  3165,    29,  1389,  6979,\n",
      "         7822,   857,  2935,    10,   102,  2911,    10,   102,  2911,\n",
      "            2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 19), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "      dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 21), dtype=int32, numpy=\n",
      "array([[    0,  6968,   216,   110,  1437, 28778,  1264,    16, 42647,\n",
      "           77,     5,  1601,   154,    15,     5,  2441,  1411,   124,\n",
      "          560,  9289,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 21), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "      dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 27), dtype=int32, numpy=\n",
      "array([[    0,    29,  9451,   259,  2445,    13,    55,    87,   457,\n",
      "         1946,    13,   127,  1553,    90,  2494, 12358, 30152,  1067,\n",
      "        28342,    59,    49,   916,    11,     5,   490,  2579,     2]],\n",
      "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 27), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 12), dtype=int32, numpy=\n",
      "array([[    0,  6460,     5,  6548, 39547,  5814,   160,     5, 15636,\n",
      "         2836,   177,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 12), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 20), dtype=int32, numpy=\n",
      "array([[    0,  9713,  4535,    23,  2248,   151, 16935,    65,  9455,\n",
      "          204,  7796,    52,   240,     5, 10535,     9,     5,   499,\n",
      "          122,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 20), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "      dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 13), dtype=int32, numpy=\n",
      "array([[    0, 24240,   428,  3894,   705,   907, 20150,  2999,  1644,\n",
      "           10,   102,  2911,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 13), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 20), dtype=int32, numpy=\n",
      "array([[    0,  9713,  4535,    23,  2248,   151, 16935,    65,  9455,\n",
      "          204,  7796,    52,   240,     5, 10535,     9,     5,   499,\n",
      "          122,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 20), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "      dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 19), dtype=int32, numpy=\n",
      "array([[    0, 17827, 39779, 13418,  1649, 31665, 27071,  1649,    13,\n",
      "           45,   145,    10,  3055, 17309,   939,  2045,     7,  7690,\n",
      "            2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 19), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "      dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 9), dtype=int32, numpy=\n",
      "array([[    0,  5016,    45, 11821,    24,    11,    42,   169,     2]],\n",
      "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 24), dtype=int32, numpy=\n",
      "array([[    0, 27326,    16,  8232,    62,     7,   592,   433, 15162,\n",
      "           16,  5947,    10,   592,   433, 25259,    11,   897,    40,\n",
      "         8746,  9143,   295,   219,   438,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 24), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 30), dtype=int32, numpy=\n",
      "array([[    0,  9713,  1240, 36300,   364,   710,    23, 15162,  1437,\n",
      "        28778,  1264,   231,  2280,   117,  1181,   173,   620,   279,\n",
      "          939,   300,   514,    11,    24,   118,   222,    45, 10538,\n",
      "         2111,   544,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 30), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 16), dtype=int32, numpy=\n",
      "array([[    0, 14746,    16,   164,     7,  1045,    41,  6662,    58,\n",
      "           52,    64,  9977,  1397,  2788,  3731,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 16), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 16), dtype=int32, numpy=\n",
      "array([[   0, 3056,   24, 1302,   14,  222,   45,  386,   11,   10, 8247,\n",
      "          71,   70,  265, 2903,    2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 16), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 16), dtype=int32, numpy=\n",
      "array([[   0, 4783, 9972,   16, 6574,  127, 5856,   99,  109, 1717,   33,\n",
      "           7,  224,   59,   42,    2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 16), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 22), dtype=int32, numpy=\n",
      "array([[    0, 14746,    16,  8201,  1345, 20021,     8,    32,  4631,\n",
      "        12173,    13,   167,    54,  3584,   716,    15,  1575,    14,\n",
      "           32,   117,    55,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 22), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "      dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 14), dtype=int32, numpy=\n",
      "array([[    0,    29,    92, 11354,   136,  3187,  8545,  2903,  1437,\n",
      "        28778,  1264,   265,  4695,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 14), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 6), dtype=int32, numpy=array([[    0,   298, 28180,   352, 29384,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 6), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 9), dtype=int32, numpy=\n",
      "array([[    0, 35901,    16,   939,   366,   406,    66,   648,     2]],\n",
      "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 14), dtype=int32, numpy=\n",
      "array([[    0,   698,   275, 11235,   548, 41207, 15162,    10,   102,\n",
      "         2911,  2903,   541,  1241,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 14), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 20), dtype=int32, numpy=\n",
      "array([[    0,  9713,  4535,    23,  2248,   151, 16935,    65,  9455,\n",
      "          204,  7796,    52,   240,     5, 10535,     9,     5,   499,\n",
      "          122,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 20), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "      dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 12), dtype=int32, numpy=\n",
      "array([[    0, 30484,     7, 20607,    62,     8,   146,    10,  1692,\n",
      "         8411, 21554,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 12), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 11), dtype=int32, numpy=\n",
      "array([[    0,   506,  1717, 11540, 12358, 30152,    64, 23829,   127,\n",
      "         8446,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 11), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 22), dtype=int32, numpy=\n",
      "array([[    0, 34462,    63,   308, 17777,  1195,    87, 11277,     5,\n",
      "         1465,    30,  5650,    15, 39127,  4399, 19633,  1741, 11465,\n",
      "        49641,   740,   991,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 22), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "      dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 21), dtype=int32, numpy=\n",
      "array([[    0, 12338,  1953,     9,   579,  5381,   475, 46328,   414,\n",
      "           53,   311,   162,     5,   299,  1964,    30,  1400,   101,\n",
      "           42,    13,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 21), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "      dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 23), dtype=int32, numpy=\n",
      "array([[    0,   757,  1256,   686,  3584,   295,  1638,  5003,   793,\n",
      "         3758, 23225,    31, 43601,    13,   939,   366,   262, 31411,\n",
      "         5241,   290, 32131,  9834,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 23), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[   0, 5016,  354,    2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[    0, 26650,   162,    42, 40123,   605,  7915,   175,   298,\n",
      "        25091,  1178,  3999,   354,  3252,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 17), dtype=int32, numpy=\n",
      "array([[    0,   118,   224,    52,   386,    10, 13069,    50,   402,\n",
      "          939,    64,  1787,     5,  3242,  1990,  2258,     2]],\n",
      "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 17), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 16), dtype=int32, numpy=\n",
      "array([[    0, 12967,    98, 26678,   155, 34143,   268,    11,     5,\n",
      "           94,   204,   688,   939,  4157,    47,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 16), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[    0, 27326, 28028,  1830,   804,  3482,    23,  7004,   135,\n",
      "           11,   201,    10,   102,  2911,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 20), dtype=int32, numpy=\n",
      "array([[    0,  3690,   159,    15,     5,  5373,  5473, 26875,  2014,\n",
      "        13550,   417,  2596,    11,     5,  1895,    31,     5, 16333,\n",
      "         2003,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 20), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "      dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 43), dtype=int32, numpy=\n",
      "array([[    0,  6350,  2591,    16,    15, 28973,   748,  4623,  5030,\n",
      "          523,  5473,  4779,   741,  2161, 13418,  6298,   821,  1951,\n",
      "         4306,    90,  6505,   543,   242,  8447,  2161,   506,   449,\n",
      "         1115,  1242,  9131,   271,  5473,   225,   821, 19112,   324,\n",
      "         1883,   263,  1350,   267,   293,  5998,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 43), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "      dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 27), dtype=int32, numpy=\n",
      "array([[    0,  1075,   571,    42,  1816,    16,   202,   634,   127,\n",
      "         1047,     7,   860,     8, 27754,     7,    69,  1760,    47,\n",
      "           33,     5,  1593,  1047,   912,   667,     7, 12881,     2]],\n",
      "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 27), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 21), dtype=int32, numpy=\n",
      "array([[    0,   627,   275,  3104,     9,     5,   183,    15,  2903,\n",
      "         8635,   139,  2458,   992, 21028,  2865,  1411,    71,  1553,\n",
      "          625, 36237,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 21), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "      dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 26), dtype=int32, numpy=\n",
      "array([[    0,  9713,  4800,  1289,  1597,   179,  3246,    11,  1400,\n",
      "           11,   295,   219,   438,    16,    24,   162,    50,    16,\n",
      "           42,  6378, 33540, 24352,   114,    47,  2854,     2]],\n",
      "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 26), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 21), dtype=int32, numpy=\n",
      "array([[    0,   118,    40,    28,  6889,    30,  2842, 13561,    77,\n",
      "           24, 15637,    10,  1907,   134, 33560, 15406, 22053,   385,\n",
      "           29,  1916,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 21), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "      dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 12), dtype=int32, numpy=\n",
      "array([[    0,  9713,   596,    16,  3999,    89,    10,   385,   241,\n",
      "        22088, 21554,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 12), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[    0,  9675, 17286, 16576,     8, 18234, 24072,  2903,   326,\n",
      "        47478,    70,    59,     5,  9219,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 21), dtype=int32, numpy=\n",
      "array([[    0,  9713, 15162, 19961,   261, 29522,  3165,    29,  1389,\n",
      "           94,  6979,  7822,   857,  2935,    10,   102,  2911,    10,\n",
      "          102,  2911,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 21), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "      dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 23), dtype=int32, numpy=\n",
      "array([[    0,  6621,  2911,  2446,     7,     5,   786, 21833, 16881,\n",
      "         7409,  6059,   388,  9305,   155,   332,    11,    65,  2289,\n",
      "         2446,   385, 38098,  8762,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 23), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1]], dtype=int32)>}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dt \u001b[38;5;129;01min\u001b[39;00m data_train:\n\u001b[0;32m----> 2\u001b[0m     encoded_data_train \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(encoded_data_train)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# encoded_data_val = tokenizer.encode_plus(\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#     data_val,\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#     add_special_tokens=True,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# values_val = torch.tensor(y_err_val[:, 0], dtype=torch.float)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# errors_val = torch.tensor(y_err_val[:, 1],dtype=torch.float)\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/Data/projects/roberta/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2982\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2972\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2973\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2974\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2975\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2979\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2980\u001b[0m )\n\u001b[0;32m-> 2982\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2985\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2989\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2990\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2991\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2992\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2993\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2995\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2996\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2997\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2999\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3000\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3001\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/Data/projects/roberta/lib/python3.11/site-packages/transformers/models/roberta/tokenization_roberta_fast.py:280\u001b[0m, in \u001b[0;36mRobertaTokenizerFast._encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m is_split_into_words \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_split_into_words\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_split_into_words, (\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with add_prefix_space=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto use it with pretokenized inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m )\n\u001b[0;32m--> 280\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/Data/projects/roberta/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:576\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    556\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    574\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    575\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 576\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m/mnt/Data/projects/roberta/lib/python3.11/site-packages/transformers/models/roberta/tokenization_roberta_fast.py:270\u001b[0m, in \u001b[0;36mRobertaTokenizerFast._batch_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m is_split_into_words \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_split_into_words\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_split_into_words, (\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with add_prefix_space=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto use it with pretokenized inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m )\n\u001b[0;32m--> 270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/Data/projects/roberta/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:504\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;66;03m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    497\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m    498\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    501\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    502\u001b[0m )\n\u001b[0;32m--> 504\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    516\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    518\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    528\u001b[0m ]\n",
      "\u001b[0;31mTypeError\u001b[0m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
     ]
    }
   ],
   "source": [
    "for dt in data_train:\n",
    "    encoded_data_train = tokenizer.encode_plus(\n",
    "        dt,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        return_tensors='tf',\n",
    "    )\n",
    "    print(encoded_data_train)\n",
    "\n",
    "# encoded_data_val = tokenizer.encode_plus(\n",
    "#     data_val,\n",
    "#     add_special_tokens=True,\n",
    "#     return_attention_mask=True,\n",
    "#     pad_to_max_length=True,\n",
    "#     max_length=512,\n",
    "#     return_tensors='pt'\n",
    "# )\n",
    "\n",
    "# input_ids_train = encoded_data_train['input_ids']\n",
    "# attention_masks_train = encoded_data_train['attention_mask']\n",
    "# values_train = torch.tensor(y_err_train[:, 0],dtype=torch.float)\n",
    "# errors_train = torch.tensor(y_err_train[:, 1],dtype=torch.float)\n",
    "\n",
    "# input_ids_val = encoded_data_val['input_ids']\n",
    "# attention_masks_val = encoded_data_val['attention_mask']\n",
    "# values_val = torch.tensor(y_err_val[:, 0], dtype=torch.float)\n",
    "# errors_val = torch.tensor(y_err_val[:, 1],dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roberta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
